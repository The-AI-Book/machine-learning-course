{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This course is based on Julien Mairal course on Kernel Methods for Machine Learning. You can find his course in the following link: https://www.youtube.com/watch?v=IzGS8uKc5E4\n",
    "\n",
    "<img src = \"https://www.researchgate.net/profile/Kiret-Dhindsa/publication/323969239/figure/fig11/AS:607404244889603@1521827865063/An-illustration-of-the-kernel-method-in-the-SVM-which-transforms-the-data-to-become.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Kernels and RKHS\n",
    "- Positive Definite Kernels\n",
    "- Reproducing Kernel Hilbert Spaces (RKHS)\n",
    "- Examples\n",
    "- Smoothness functional\n",
    "\n",
    "## 2. Kernel tricks\n",
    "- The kernel trick\n",
    "- The representer theorem\n",
    "\n",
    "## 3. Kernel methods: Supervised Learning\n",
    "- Kernel ridge regression\n",
    "- Kernel logistic regression\n",
    "- Large-mmargin classifiers\n",
    "- Interlude: convex optimiacion and duality\n",
    "- Support vector machines\n",
    "\n",
    "## 4. Kernel methods: Unsupervised Learning\n",
    "- Kernel PCA\n",
    "- Kernel K-meansand spectral clustering\n",
    "- A quick note on kernel CCA\n",
    "\n",
    "## 5. The Kernel Jungle\n",
    "- Green, Mercer, Herglotz, Bochner and frined\n",
    "- Kernels for probabilistic models\n",
    "- Kernels for biological sequences\n",
    "- Kernels for graphs\n",
    "- Kernels on graphs\n",
    "\n",
    "## 6. Open Problems and Research Topics\n",
    "- Multiple Kernel Learning (MKL)\n",
    "- Large-scales learning with kernels\n",
    "- Foundations of deep learning from a kenerl point of view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kernel Method.\n",
    "\n",
    "---\n",
    "\n",
    "In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM).  The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets.\n",
    "\n",
    "For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.\n",
    "\n",
    "Kernel methods owe their name to the use of **kernel functions**, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the \"kernel trick\".\n",
    "\n",
    "Some algorithms capable of operating with kernels include: \n",
    "\n",
    "* Kernel perceptron\n",
    "* Support vector machine (SVM)\n",
    "* Gaussan processes\n",
    "* Principal Components Analysis\n",
    "* Canonical Correlation Analysis\n",
    "* Ridge Regression\n",
    "* Spectral Clustering\n",
    "* Linear adaptive filters\n",
    "* and many others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But, what is a kernel function?\n",
    "\n",
    "In operator theory, a branch of mathematics, a positive-definite kernel is a generalization of a positive-definite function or a positive-definite matrix. It was first introduced by James Mercer in the early 20th century, in the context of solving integral operator equations. Since then positive-definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics. They occur naturally in Fourier analysis, probability theory, operator theory, complex function-theory, moment problems, integral equations, boundary-value problems for partial differential equations, machine learning, embedding problem, information theory, and other areas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Let $X$ be a nonempty set, sometimes referred to as the index set. A symmetric function {\\displaystyle K:{\\mathcal {X}}\\times {\\mathcal {X}}\\to \\mathbb {R} }{\\displaystyle K:{\\mathcal {X}}\\times {\\mathcal {X}}\\to \\mathbb {R} } is called a positive-definite (p.d.) kernel on $X$ if\n",
    "\n",
    "$$\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n c_i c_j K(x_i, x_j) \\geq 0 $$\n",
    "\n",
    "holds for any $x_1, \\dots, x_n \\in X$, given $n \\in \\mathbb{N}, c_1, \\dots, c_n \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
