{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This course is based on Julien Mairal course on Kernel Methods for Machine Learning. This notebook corresponds to the first class of the course. You can find the slides in the following link: https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear kernel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take $X = \\mathbb{R}^d$ and the linear kernel: \n",
    "\n",
    "$$K(x, y) = \\langle x, y \\rangle_{\\mathbb{R}^d}  $$\n",
    "\n",
    "### Theorem\n",
    "\n",
    "The RKHS of the linear kernel is the set of linear functions of the form ($f = K$)\n",
    "\n",
    "$$f_{w}(x) = \\langle w, x \\rangle_{\\mathbb{R}^d} \\quad \\text{for} \\quad w \\in \\mathbb{R}^d, $$\n",
    "\n",
    "endowed with the inner product \n",
    "\n",
    "$$ \\forall w, v \\in \\mathbb{R}^d, \\quad \\langle f_w, f_v \\rangle_{H} = f_{w}(v) = \\langle w, v \\rangle_{\\mathbb{R}^d} $$\n",
    "\n",
    "and corresponding norm\n",
    "\n",
    "$$ \\forall w \\in \\mathbb{R}^d, \\quad ||f_w||_{H} = ||w||_2 $$\n",
    "\n",
    "#### Note (Algebraic dual space)\n",
    "\n",
    "Given any vector space $V$ over a field $F$, the (algebraic) dual space $V^{*}$ is defined as the set of all linear maps $\\phi: V \\rightarrow F$ (linear functionals). Since linear maps are vector space homomorphisms, the dual space may be denoted $hom(V, F)$. The dual space $V^{*}$ itself becomes a vector space over $F$ when equipped with an addition and scalar multiplication satisfying: \n",
    "\n",
    "$$ (\\phi + \\theta)(x) = \\phi(x) + \\theta(x) $$\n",
    "$$ (a \\phi)(x) = a(\\phi(x))  $$\n",
    "for all $\\phi, \\theta \\in V^{*}, x \\in V$ and $a \\in F$\n",
    "\n",
    "**By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the oriuginal space.**\n",
    "\n",
    "### Proof\n",
    "\n",
    "The set $H$ of functions described in the theorem is the dual of $\\mathbb{R}^d$. hence it is a Hilbert space:\n",
    "\n",
    "$$ H = \\{ f_{w}(x) = \\langle w, x \\rangle_{\\mathbb{R}^d} : w \\in \\mathbb{R}^d \\}$$\n",
    "\n",
    "- $H$ contains all functions of the form $K_w: x \\rightarrow \\langle w, x \\rangle_{\\mathbb{R}^d}$\n",
    "- For every $x \\in \\mathbb{R}^d$ and $f_w \\in H$, \n",
    "$$ f_w(x) = \\langle w, x \\rangle_{\\mathbb{R}^d} = \\langle f_w, K_x \\rangle_{H}$$\n",
    "\n",
    "$H$ is thus the RKHS of the linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Linear kernel\n",
    "def linear_kernel(x, y):\n",
    "    return np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernel\n",
    "\n",
    "---\n",
    "\n",
    "Take $X = \\mathbb{R}^d$ and the polynomial kernel of degree 2: \n",
    "\n",
    "$$K(x, y) = \\langle x, y \\rangle_{\\mathbb{R}^d}^2 = (x^T y)^2 = \\textsf{trace}(x^Ty x^Ty) = \\langle xx^T, yy^T \\rangle_F $$\n",
    "\n",
    "where $F$ is the Froebenius norm for matrices in $\\mathbb{R}^{d \\times d}$. Note that we have proven here that $K$ is p.d.\n",
    "\n",
    "### Proof\n",
    "\n",
    "Check slides 45 and 46.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel of degree 2.\n",
    "def polynomial_kernel_degree_2(x, y):\n",
    "    return np.dot(np.transpose(x) * y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: what is the RKHS of the general polynomial kernel?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel_p(x, y, p):\n",
    "    return np.dot(np.transpose(x) * y)**p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining kernels\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem \n",
    "\n",
    "- If $K_1$ and $K_2$ are p.d kernels, then: \n",
    "\n",
    "$$K_1 + K_2, K_1 K_2 \\quad \\text{and} \\quad cK_1, \\quad \\text{for} \\quad c \\geq 0 $$\n",
    "are also p.d kernels\n",
    "\n",
    "- If $(K_i)_{i \\geq 1}$ is a sequence of p.d kernels that converges pointwisely to a function $K$: \n",
    "\n",
    "$$\\forall (x, x^{'}) \\in X^2, \\quad K(x, x^{'}) = \\lim_{n \\rightarrow \\infty} K_i (x, x^{'}), $$\n",
    "\n",
    "then $K$ is also a p.d. kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Theorem\n",
    "\n",
    "If $K$ is a kernel, then $e^K$ is a kernel too.\n",
    "\n",
    "### Proof\n",
    "\n",
    "$$ \\exp^{K(x, x^{'})} = \\lim_{n \\rightarrow + \\infty} \\sum\\limits_{i = 0}^n \\frac{K(x, x^{'})^i}{i !}$$\n",
    "\n",
    "is p.d. This latter, follow from the propeties of the previous theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "Which of the followin are p.d kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathcal{X}=(-1,1), \\quad K\\left(x, x^{\\prime}\\right)=\\frac{1}{1-x x^{\\prime}}$\n",
    "\n",
    "- $\\mathcal{X}=\\mathbb{N}, \\quad K\\left(x, x^{\\prime}\\right)=2^{x+x^{\\prime}}$\n",
    "\n",
    "- $\\mathcal{X}=\\mathbb{N}, \\quad K\\left(x, x^{\\prime}\\right)=2^{x x^{\\prime}}$\n",
    "\n",
    "- $\\mathcal{X}=\\mathbb{R}_{+}, \\quad K\\left(x, x^{\\prime}\\right)=\\log \\left(1+x x^{\\prime}\\right)$\n",
    "\n",
    "-  $\\mathcal{X}=\\mathbb{R}, \\quad K\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\left|x-x^{\\prime}\\right|^{2}\\right)$\n",
    "\n",
    "- $\\mathcal{X}=\\mathbb{R}, \\quad K\\left(x, x^{\\prime}\\right)=\\cos \\left(x+x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{R}, \\quad K\\left(x, x^{\\prime}\\right)=\\cos \\left(x-x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{R}_{+}, \\quad K\\left(x, x^{\\prime}\\right)=\\min \\left(x, x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{R}_{+}, \\quad K\\left(x, x^{\\prime}\\right)=\\max \\left(x, x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{R}_{+}, \\quad K\\left(x, x^{\\prime}\\right)=\\min \\left(x, x^{\\prime}\\right) / \\max \\left(x, x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{N}, \\quad K\\left(x, x^{\\prime}\\right)=\\operatorname{GCD}\\left(x, x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{N}, \\quad K\\left(x, x^{\\prime}\\right)=\\operatorname{LCM}\\left(x, x^{\\prime}\\right)$\n",
    "- $\\mathcal{X}=\\mathbb{N}, \\quad K\\left(x, x^{\\prime}\\right)=\\operatorname{GCD}\\left(x, x^{\\prime}\\right) / \\operatorname{LCM}\\left(x, x^{\\prime}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothness functional\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small norm $\\Longrightarrow$ slow variations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
